{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/baonguyen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/baonguyen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"/Users/baonguyen/IU/thesis/data/raw_data/renttherunway_final_data.json\",lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take just the content of the article, lowercase and remove punctuation\n",
    "summaries = data['review_text'].str.lower().apply(lambda x: re.sub(r\"([^\\w\\s])\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop word removal\n",
    "en_stopwords = stopwords.words('english')\n",
    "summaries = summaries.apply(lambda x: ' '.join([word for word in x.split() if word not in (en_stopwords)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 6017/6017 [03:49<00:00, 26.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# 5. Generate sentence embeddings using all-MiniLM-L6-v2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(summaries, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baonguyen/.pyenv/versions/thesis-env/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/baonguyen/.pyenv/versions/thesis-env/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "# 6. UMAP for dimensionality reduction to 2D\n",
    "import umap\n",
    "reducer = umap.UMAP(n_components=16, random_state=42)\n",
    "umap_embeddings = reducer.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# 7. HDBSCAN clustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "clusterer = DBSCAN(min_samples=3)\n",
    "labels = clusterer.fit_predict(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Topic'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 21:43:02,859 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 6017/6017 [03:27<00:00, 29.04it/s]\n",
      "2025-04-14 21:46:41,954 - BERTopic - Embedding - Completed ✓\n",
      "2025-04-14 21:46:41,954 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-04-14 21:47:55,556 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-14 21:47:55,560 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2025-04-14 21:48:08,835 - BERTopic - Cluster - Completed ✓\n",
      "2025-04-14 21:48:08,836 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-04-14 21:48:23,899 - BERTopic - Representation - Completed ✓\n",
      "2025-04-14 21:48:23,918 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-04-14 21:48:27,254 - BERTopic - Topic reduction - Reduced number of topics from 666 to 15\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.517098040582251\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "# Fine-tune your topic representations\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# Convert doc_term to a list of strings\n",
    "docs = []\n",
    "for doc in doc_term:\n",
    "    doc_str = ' '.join([dictionary[word_id] for word_id, _ in doc])\n",
    "    print(doc_str)\n",
    "    docs.append(doc_str)\n",
    "\n",
    "# Fit BERTopic model\n",
    "topic_model = BERTopic(embedding_model='sentence-transformers/all-MiniLM-L6-v2',nr_topics=15,verbose=True,representation_model=representation_model)\n",
    "topics, probabilities = topic_model.fit_transform(docs)\n",
    "\n",
    "# Preprocess documents\n",
    "cleaned_docs = topic_model._preprocess_text(docs)\n",
    "\n",
    "# Extract vectorizer and tokenizer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "tokenizer = vectorizer.build_tokenizer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [tokenizer(doc) for doc in cleaned_docs]\n",
    "dictionary = Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "\n",
    "# Extract topic words\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)]\n",
    "               for topic in range(len(set(topics)) - 1)]\n",
    "\n",
    "# Evaluate coherence\n",
    "coherence_model = CoherenceModel(topics=topic_words,\n",
    "                                 texts=tokens,\n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary,\n",
    "                                 coherence='c_v')\n",
    "coherence = coherence_model.get_coherence()\n",
    "\n",
    "print(f\"Coherence Score: {coherence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add topics to the original DataFrame\n",
    "data['Topic'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/clean_data/data_with_bertopic_column_.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
