{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import copy\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "embed_dim = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/baonguyen/IU/thesis/data/clean_data/data_with_bertopic_column.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_date']=pd.to_datetime(df['review_date'])\n",
    "df_sorted = df.sort_values('review_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_item_id = set(df_sorted['item_id'])\n",
    "item_to_index = {item:idx +1 for idx , item in enumerate(unique_item_id)}\n",
    "index_to_item = {idx+1:item for idx , item in enumerate(unique_item_id)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group and aggregate\n",
    "user_item_sequence = (\n",
    "    df_sorted.groupby('user_id')[['item_id']]\n",
    "    .agg(list)\n",
    "    .to_dict(orient='index')\n",
    ")\n",
    "\n",
    "# Step 2: Remove users with fewer than 2 item_ids\n",
    "user_item_sequence = {\n",
    "    user: val\n",
    "    for user, val in user_item_sequence.items()\n",
    "    if len(val['item_id']) >= 2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_to_index_sequence = {}\n",
    "for user,value in user_item_sequence.items():\n",
    "    user_item_to_index_sequence[user] = {'item_id':[item_to_index[item] for item in value['item_id']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def mask_sequence(sequence: dict, mask_ratio: float):\n",
    "    labels = {}\n",
    "    mask_seq = {}\n",
    "    for user, seq in sequence.items():\n",
    "        mask_seq[user] = copy.deepcopy(seq)  # Deep copy so original is untouched\n",
    "        labels[user] = [-100] * len(seq['item_id'])\n",
    "        for i in range(len(mask_seq[user]['item_id'])):\n",
    "            if random.random() < mask_ratio:\n",
    "                labels[user][i] = mask_seq[user]['item_id'][i]  # Save original item id\n",
    "                mask_seq[user]['item_id'][i] = 0       # Mask the item id\n",
    "               \n",
    "    return mask_seq, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(mask_seq, labels, max_len=64, pad_item=0, pad_topic=0, pad_label=-100):\n",
    "    \"\"\"\n",
    "    Pads all user sequences in mask_seq and labels to max_len.\n",
    "    \n",
    "    Args:\n",
    "        mask_seq: dict of user_id -> {'item_id': [...], 'Topic': [...]}\n",
    "        labels: dict of user_id -> [...]\n",
    "        max_len: desired length after padding\n",
    "        pad_item: value for padding 'item_id'\n",
    "        pad_topic: value for padding 'Topic'\n",
    "        pad_label: value for padding labels\n",
    "\n",
    "    Returns:\n",
    "        padded_mask_seq, padded_labels (dicts)\n",
    "    \"\"\"\n",
    "    def pad(seq, max_len, pad_value):\n",
    "        if len(seq) < max_len:\n",
    "            return seq + [pad_value] * (max_len - len(seq))\n",
    "        else:\n",
    "            return seq[len(seq)-max_len:len(seq)]\n",
    "    \n",
    "    padded_mask_seq = {}\n",
    "    padded_labels = {}\n",
    "\n",
    "    for user in mask_seq:\n",
    "        padded_mask_seq[user] = {\n",
    "            'item_id': pad(mask_seq[user]['item_id'], max_len, pad_item)\n",
    "        }\n",
    "        padded_labels[user] = pad(labels[user], max_len, pad_label)\n",
    "    \n",
    "    return padded_mask_seq, padded_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert architect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self,vocab_size , hidden_size,max_len,dropout):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.word_embeddings = nn.Embedding(vocab_size,hidden_size)\n",
    "        self.position_encoding = nn.Embedding(max_len,hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size)\n",
    "        self.Dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,input_ids):\n",
    "        position_ids = torch.arange(self.max_len, dtype=torch.long, device=input_ids.device).unsqueeze(0)\n",
    "        word_emb = self.word_embeddings(input_ids)\n",
    "        pos_emb = self.position_encoding(position_ids)\n",
    "        embeddings = word_emb + pos_emb\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        return self.Dropout(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class BertSdpaSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size=512, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attention_mask=None,key_padding_mask=None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # Linear projection and reshape\n",
    "        q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if attention_mask is not None:\n",
    "            scores += attention_mask\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(key_padding_mask,float('-inf'))\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = torch.nan_to_num(attn_weights, nan=0.0)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        context = torch.matmul(attn_weights, v)  # [B, H, T, D]\n",
    "        context = context.transpose(1, 2).reshape(B, T, C)\n",
    "        return context\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, hidden_size=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        return self.LayerNorm(hidden_states + input_tensor)\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, hidden_size=512, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self = BertSdpaSelfAttention(hidden_size, num_heads, dropout)\n",
    "        self.output = BertSelfOutput(hidden_size, dropout)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None,key_padding_mask=None):\n",
    "        self_output = self.self(hidden_states, attention_mask,key_padding_mask)\n",
    "        return self.output(self_output, hidden_states)\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, hidden_size=512, intermediate_size=3072):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        return self.activation(self.dense(hidden_states))\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, intermediate_size=3072, hidden_size=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        return self.LayerNorm(hidden_states + input_tensor)\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, hidden_size=512, intermediate_size=3072, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = BertAttention(hidden_size, num_heads, dropout)\n",
    "        self.intermediate = BertIntermediate(hidden_size, intermediate_size)\n",
    "        self.output = BertOutput(intermediate_size, hidden_size, dropout)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None,key_padding_mask=None):\n",
    "        attention_output = self.attention(hidden_states, attention_mask,key_padding_mask)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, num_layers=2, hidden_size=512, intermediate_size=3072, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layer = nn.ModuleList([\n",
    "            BertLayer(hidden_size, intermediate_size, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None,key_padding_mask=None):\n",
    "        for layer_module in self.layer:\n",
    "            hidden_states = layer_module(hidden_states, attention_mask,key_padding_mask)\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class BertModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size=30522,\n",
    "                 hidden_size=512,\n",
    "                 intermediate_size=3072,\n",
    "                 num_heads=8,\n",
    "                 num_layers=2,\n",
    "                 max_len=512,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embeddings = BertEmbeddings(vocab_size, hidden_size, max_len, dropout=dropout)\n",
    "        self.encoder = BertEncoder(num_layers, hidden_size, intermediate_size, num_heads, dropout)\n",
    "        self.output_layer = nn.Sequential(\n",
    "           nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None,key_padding_mask=None):\n",
    "        \n",
    "\n",
    "        embedding_output = self.embeddings(input_ids)\n",
    "        encoder_output = self.encoder(embedding_output, attention_mask,key_padding_mask)\n",
    "        output = self.output_layer(encoder_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "model = BertModel()\n",
    "summary(model,depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_ratio(ground_truth:list,prediction:list,k:int):\n",
    "    hits = 0\n",
    "    total = len(ground_truth)\n",
    "    for gt_item, pred in zip(ground_truth, prediction):\n",
    "        if gt_item in pred[:k]:\n",
    "            hits += 1\n",
    "    return hits / total\n",
    "\n",
    "# -------------------------\n",
    "def evaluate_model(model, val_item_sequences, k=10):\n",
    "    model.eval()\n",
    "    device = 'mps'\n",
    "\n",
    "    ground_truths = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for user, seq in val_item_sequences.items():\n",
    "            item_seq = seq['item_id']\n",
    "            \n",
    "\n",
    "\n",
    "            # Prepare input and target\n",
    "            input_items = item_seq[:-1]\n",
    "            \n",
    "            target_item = item_seq[-1]\n",
    "\n",
    "            # Use your own padding utility to ensure correct length\n",
    "            padded_seq, _ = padding(\n",
    "                mask_seq={user: {'item_id': input_items}},\n",
    "                labels={user: []},  # empty labels not needed here\n",
    "                max_len=max_len\n",
    "            )\n",
    "            \n",
    "            padded_items = padded_seq[user]['item_id']\n",
    "            # padded_topics = padded_seq[user]['Topic']\n",
    "\n",
    "            item_tensor = torch.tensor([padded_items], dtype=torch.long).to(device)\n",
    "            \n",
    "            key_padding_mask = (item_tensor == 0)\n",
    "\n",
    "            logits = model(item_tensor, key_padding_mask=key_padding_mask)[:,min(len(item_seq)-1,max_len-1),:]\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            # print(probabilities.size())\n",
    "            \n",
    "            \n",
    "\n",
    "            topk = torch.topk(probabilities, k=k).indices[0].tolist()\n",
    "            # print(topk)\n",
    "            ground_truths.append(index_to_item[target_item])\n",
    "            predictions.append([index_to_item[i] for i in topk])\n",
    "\n",
    "            # print(ground_truths)\n",
    "            # print(predictions)\n",
    "    return hit_ratio(ground_truths, predictions, k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "def train_model(train_users,val_users,fold_num):\n",
    "    train_user_item_to_index_sequence = {user: seq for user, seq in user_item_to_index_sequence.items() if user in train_users}\n",
    "    mask_seq , labels = mask_sequence(train_user_item_to_index_sequence,mask_ratio=0.6)\n",
    "    padded_mask_seq,padded_labels = padding(mask_seq,labels,max_len=max_len)\n",
    "\n",
    "    tensor_item_ids = torch.stack([\n",
    "        torch.tensor(user_seq['item_id']) for user_seq in padded_mask_seq.values()\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    tensor_labels = torch.stack([\n",
    "        torch.tensor(seq) for seq in padded_labels.values()\n",
    "        ])\n",
    "        \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        tensor_item_ids,\n",
    "\n",
    "        tensor_labels\n",
    "    )\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
    "\n",
    "\n",
    "    # train model \n",
    "    device = 'mps'\n",
    "    model = BertModel(vocab_size=len(unique_item_id)+1,hidden_size=256,intermediate_size=256*12,num_heads=4,num_layers=2,max_len=50).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    best_hr = 0\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch in tqdm(train_dataloader,desc=f\"Fold {fold_num} Epoch {epoch+1}\", unit=\"batch\"):\n",
    "            item_ids  , labels = batch\n",
    "            item_ids  , labels = item_ids.to(device) , labels.to(device)\n",
    "            key_padding_mask = (item_ids == 0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(item_ids,key_padding_mask=key_padding_mask)\n",
    "            # print(outputs.size()\n",
    "            loss = criterion(outputs.view(-1, len(unique_item_id)+1), labels.view(-1))\n",
    "            loss.backward()\n",
    "            torch.mps.empty_cache()\n",
    "            optimizer.step()\n",
    "            # print(loss.item())\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss {epoch_loss}\")\n",
    "        # evaluate model\n",
    "        model.eval()\n",
    "        # val_user_item_sequences = {user: seq for user, seq in user_item_sequence.items() if user in val_users}\n",
    "        val_item_sequences = {user: seq for user, seq in user_item_to_index_sequence.items() if user in val_users}\n",
    "        val_hr = evaluate_model(model, val_item_sequences,k=10)\n",
    "        print(f\"Fold {fold_num} Epoch {epoch+1}, Validation HR@10: {val_hr}\")\n",
    "        if val_hr > best_hr:\n",
    "            best_hr = val_hr\n",
    "            save_path = f\"models_item_with_bert/fold_{fold_num}\"\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"{save_path}/best_model.pth\")\n",
    "    return model, best_hr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "set_seed()\n",
    "kf = KFold(n_splits=5,shuffle=True,random_state=42)\n",
    "user_list = list(user_item_sequence.keys())\n",
    "fold_results = {}\n",
    "for fold_num , (train_idx,val_idx) in enumerate(kf.split(user_list),1):\n",
    "    print(f\"\\nStarting Fold {fold_num}...\")\n",
    "    train_users = [user_list[i] for i in train_idx]\n",
    "    val_users = [user_list[i] for i in val_idx]\n",
    "    model,val_hr =  train_model(train_users, val_users, fold_num)\n",
    "    fold_results[fold_num] = val_hr\n",
    "    save_path = f\"results_item_with_bert/fold_{fold_num}\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    with open(f\"{save_path}/results.txt\", \"w\") as f:\n",
    "        f.write(f\"Validation HR@10: {val_hr}\\n\")\n",
    "    del model\n",
    "    torch.mps.empty_cache()\n",
    "with open(\"results_item_with_bert/overall_results.txt\", \"w\") as f:\n",
    "    for fold, hr in fold_results.items():\n",
    "        f.write(f\"Fold {fold}: HR@10 = {hr}\\n\")\n",
    "    mean_hr = sum(fold_results.values()) / len(fold_results)\n",
    "    f.write(f\"\\nMean HR@10 across folds: {mean_hr}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
