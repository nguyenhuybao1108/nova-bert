{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"/Users/baonguyen/IU/thesis/data/raw_data/renttherunway_final_data.json\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit</th>\n",
       "      <th>user_id</th>\n",
       "      <th>bust size</th>\n",
       "      <th>item_id</th>\n",
       "      <th>weight</th>\n",
       "      <th>rating</th>\n",
       "      <th>rented for</th>\n",
       "      <th>review_text</th>\n",
       "      <th>body type</th>\n",
       "      <th>review_summary</th>\n",
       "      <th>category</th>\n",
       "      <th>height</th>\n",
       "      <th>size</th>\n",
       "      <th>age</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fit</td>\n",
       "      <td>420272</td>\n",
       "      <td>34d</td>\n",
       "      <td>2260466</td>\n",
       "      <td>137lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>vacation</td>\n",
       "      <td>An adorable romper! Belt and zipper were a lit...</td>\n",
       "      <td>hourglass</td>\n",
       "      <td>So many compliments!</td>\n",
       "      <td>romper</td>\n",
       "      <td>5' 8\"</td>\n",
       "      <td>14</td>\n",
       "      <td>28.0</td>\n",
       "      <td>April 20, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fit</td>\n",
       "      <td>273551</td>\n",
       "      <td>34b</td>\n",
       "      <td>153475</td>\n",
       "      <td>132lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>other</td>\n",
       "      <td>I rented this dress for a photo shoot. The the...</td>\n",
       "      <td>straight &amp; narrow</td>\n",
       "      <td>I felt so glamourous!!!</td>\n",
       "      <td>gown</td>\n",
       "      <td>5' 6\"</td>\n",
       "      <td>12</td>\n",
       "      <td>36.0</td>\n",
       "      <td>June 18, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fit</td>\n",
       "      <td>360448</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1063761</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>party</td>\n",
       "      <td>This hugged in all the right places! It was a ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It was a great time to celebrate the (almost) ...</td>\n",
       "      <td>sheath</td>\n",
       "      <td>5' 4\"</td>\n",
       "      <td>4</td>\n",
       "      <td>116.0</td>\n",
       "      <td>December 14, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fit</td>\n",
       "      <td>909926</td>\n",
       "      <td>34c</td>\n",
       "      <td>126335</td>\n",
       "      <td>135lbs</td>\n",
       "      <td>8.0</td>\n",
       "      <td>formal affair</td>\n",
       "      <td>I rented this for my company's black tie award...</td>\n",
       "      <td>pear</td>\n",
       "      <td>Dress arrived on time and in perfect condition.</td>\n",
       "      <td>dress</td>\n",
       "      <td>5' 5\"</td>\n",
       "      <td>8</td>\n",
       "      <td>34.0</td>\n",
       "      <td>February 12, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fit</td>\n",
       "      <td>151944</td>\n",
       "      <td>34b</td>\n",
       "      <td>616682</td>\n",
       "      <td>145lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>wedding</td>\n",
       "      <td>I have always been petite in my upper body and...</td>\n",
       "      <td>athletic</td>\n",
       "      <td>Was in love with this dress !!!</td>\n",
       "      <td>gown</td>\n",
       "      <td>5' 9\"</td>\n",
       "      <td>12</td>\n",
       "      <td>27.0</td>\n",
       "      <td>September 26, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192539</th>\n",
       "      <td>fit</td>\n",
       "      <td>66386</td>\n",
       "      <td>34dd</td>\n",
       "      <td>2252812</td>\n",
       "      <td>140lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>work</td>\n",
       "      <td>Fit like a glove!</td>\n",
       "      <td>hourglass</td>\n",
       "      <td>LOVE IT!!! First Item Im thinking of buying!</td>\n",
       "      <td>jumpsuit</td>\n",
       "      <td>5' 9\"</td>\n",
       "      <td>8</td>\n",
       "      <td>42.0</td>\n",
       "      <td>May 18, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192540</th>\n",
       "      <td>fit</td>\n",
       "      <td>118398</td>\n",
       "      <td>32c</td>\n",
       "      <td>682043</td>\n",
       "      <td>100lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>work</td>\n",
       "      <td>The pattern contrast on this dress is really s...</td>\n",
       "      <td>petite</td>\n",
       "      <td>LOVE it!</td>\n",
       "      <td>dress</td>\n",
       "      <td>5' 1\"</td>\n",
       "      <td>4</td>\n",
       "      <td>29.0</td>\n",
       "      <td>September 30, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192541</th>\n",
       "      <td>fit</td>\n",
       "      <td>47002</td>\n",
       "      <td>36a</td>\n",
       "      <td>683251</td>\n",
       "      <td>135lbs</td>\n",
       "      <td>6.0</td>\n",
       "      <td>everyday</td>\n",
       "      <td>Like the other DVF wraps, the fit on this is f...</td>\n",
       "      <td>straight &amp; narrow</td>\n",
       "      <td>Loud patterning, flattering fit</td>\n",
       "      <td>dress</td>\n",
       "      <td>5' 8\"</td>\n",
       "      <td>8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>March 4, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192542</th>\n",
       "      <td>fit</td>\n",
       "      <td>961120</td>\n",
       "      <td>36c</td>\n",
       "      <td>126335</td>\n",
       "      <td>165lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>wedding</td>\n",
       "      <td>This dress was PERFECTION.  it looked incredib...</td>\n",
       "      <td>pear</td>\n",
       "      <td>loved this dress it was comfortable and photog...</td>\n",
       "      <td>dress</td>\n",
       "      <td>5' 6\"</td>\n",
       "      <td>16</td>\n",
       "      <td>31.0</td>\n",
       "      <td>November 25, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192543</th>\n",
       "      <td>fit</td>\n",
       "      <td>123612</td>\n",
       "      <td>36b</td>\n",
       "      <td>127865</td>\n",
       "      <td>155lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>wedding</td>\n",
       "      <td>This dress was wonderful! I had originally pla...</td>\n",
       "      <td>athletic</td>\n",
       "      <td>I wore this to a beautiful black tie optional ...</td>\n",
       "      <td>gown</td>\n",
       "      <td>5' 6\"</td>\n",
       "      <td>16</td>\n",
       "      <td>30.0</td>\n",
       "      <td>August 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192544 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        fit  user_id bust size  item_id  weight  rating     rented for  \\\n",
       "0       fit   420272       34d  2260466  137lbs    10.0       vacation   \n",
       "1       fit   273551       34b   153475  132lbs    10.0          other   \n",
       "2       fit   360448       NaN  1063761     NaN    10.0          party   \n",
       "3       fit   909926       34c   126335  135lbs     8.0  formal affair   \n",
       "4       fit   151944       34b   616682  145lbs    10.0        wedding   \n",
       "...     ...      ...       ...      ...     ...     ...            ...   \n",
       "192539  fit    66386      34dd  2252812  140lbs    10.0           work   \n",
       "192540  fit   118398       32c   682043  100lbs    10.0           work   \n",
       "192541  fit    47002       36a   683251  135lbs     6.0       everyday   \n",
       "192542  fit   961120       36c   126335  165lbs    10.0        wedding   \n",
       "192543  fit   123612       36b   127865  155lbs    10.0        wedding   \n",
       "\n",
       "                                              review_text          body type  \\\n",
       "0       An adorable romper! Belt and zipper were a lit...          hourglass   \n",
       "1       I rented this dress for a photo shoot. The the...  straight & narrow   \n",
       "2       This hugged in all the right places! It was a ...                NaN   \n",
       "3       I rented this for my company's black tie award...               pear   \n",
       "4       I have always been petite in my upper body and...           athletic   \n",
       "...                                                   ...                ...   \n",
       "192539                                  Fit like a glove!          hourglass   \n",
       "192540  The pattern contrast on this dress is really s...             petite   \n",
       "192541  Like the other DVF wraps, the fit on this is f...  straight & narrow   \n",
       "192542  This dress was PERFECTION.  it looked incredib...               pear   \n",
       "192543  This dress was wonderful! I had originally pla...           athletic   \n",
       "\n",
       "                                           review_summary  category height  \\\n",
       "0                                    So many compliments!    romper  5' 8\"   \n",
       "1                                 I felt so glamourous!!!      gown  5' 6\"   \n",
       "2       It was a great time to celebrate the (almost) ...    sheath  5' 4\"   \n",
       "3        Dress arrived on time and in perfect condition.      dress  5' 5\"   \n",
       "4                         Was in love with this dress !!!      gown  5' 9\"   \n",
       "...                                                   ...       ...    ...   \n",
       "192539       LOVE IT!!! First Item Im thinking of buying!  jumpsuit  5' 9\"   \n",
       "192540                                           LOVE it!     dress  5' 1\"   \n",
       "192541                    Loud patterning, flattering fit     dress  5' 8\"   \n",
       "192542  loved this dress it was comfortable and photog...     dress  5' 6\"   \n",
       "192543  I wore this to a beautiful black tie optional ...      gown  5' 6\"   \n",
       "\n",
       "        size    age         review_date  \n",
       "0         14   28.0      April 20, 2016  \n",
       "1         12   36.0       June 18, 2013  \n",
       "2          4  116.0   December 14, 2015  \n",
       "3          8   34.0   February 12, 2014  \n",
       "4         12   27.0  September 26, 2016  \n",
       "...      ...    ...                 ...  \n",
       "192539     8   42.0        May 18, 2016  \n",
       "192540     4   29.0  September 30, 2016  \n",
       "192541     8   31.0       March 4, 2016  \n",
       "192542    16   31.0   November 25, 2015  \n",
       "192543    16   30.0     August 29, 2017  \n",
       "\n",
       "[192544 rows x 15 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take just the content of the article, lowercase and remove punctuation\n",
    "summaries = data['text'].str.lower().apply(lambda x: re.sub(r\"([^\\w\\s])\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop word removal\n",
    "en_stopwords = stopwords.words('english')\n",
    "summaries = summaries.apply(lambda x: ' '.join([word for word in x.split() if word not in (en_stopwords)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "summaries = summaries.apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming (done for speed as we have a lot of text)\n",
    "ps = PorterStemmer()\n",
    "sumsummaries =summaries.apply(lambda tokens: [ps.stem(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [adorable, romper, belt, zipper, little, hard,...\n",
       "1         [rented, dress, photo, shoot, theme, hollywood...\n",
       "2         [hugged, right, places, perfect, dress, event,...\n",
       "3         [rented, companys, black, tie, awards, banquet...\n",
       "4         [always, petite, upper, body, extremely, athle...\n",
       "                                ...                        \n",
       "192539                                   [fit, like, glove]\n",
       "192540    [pattern, contrast, dress, really, stunning, u...\n",
       "192541    [like, dvf, wraps, fit, fantastic, albeit, col...\n",
       "192542    [dress, perfection, looked, incredible, photos...\n",
       "192543    [dress, wonderful, originally, planned, wear, ...\n",
       "Name: review_text, Length: 192544, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<48569 unique tokens: ['absolutely', 'adorable', 'belt', 'compliments', 'day']...>\n"
     ]
    }
   ],
   "source": [
    "# create dictionary of all words\n",
    "dictionary = corpora.Dictionary(summaries)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vecotize using bag of words into a document term matrix\n",
    "doc_term = [dictionary.doc2bow(text) for text in summaries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 21:43:02,859 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6017/6017 [03:27<00:00, 29.04it/s]\n",
      "2025-04-14 21:46:41,954 - BERTopic - Embedding - Completed âœ“\n",
      "2025-04-14 21:46:41,954 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-04-14 21:47:55,556 - BERTopic - Dimensionality - Completed âœ“\n",
      "2025-04-14 21:47:55,560 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2025-04-14 21:48:08,835 - BERTopic - Cluster - Completed âœ“\n",
      "2025-04-14 21:48:08,836 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-04-14 21:48:23,899 - BERTopic - Representation - Completed âœ“\n",
      "2025-04-14 21:48:23,918 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-04-14 21:48:27,254 - BERTopic - Topic reduction - Reduced number of topics from 666 to 15\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.517098040582251\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "# Fine-tune your topic representations\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# Convert doc_term to a list of strings\n",
    "docs = []\n",
    "for doc in doc_term:\n",
    "    doc_str = ' '.join([dictionary[word_id] for word_id, _ in doc])\n",
    "    docs.append(doc_str)\n",
    "\n",
    "# Fit BERTopic model\n",
    "topic_model = BERTopic(embedding_model='sentence-transformers/all-MiniLM-L6-v2',nr_topics=15,verbose=True,representation_model=representation_model)\n",
    "topics, probabilities = topic_model.fit_transform(docs)\n",
    "\n",
    "# Preprocess documents\n",
    "cleaned_docs = topic_model._preprocess_text(docs)\n",
    "\n",
    "# Extract vectorizer and tokenizer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "tokenizer = vectorizer.build_tokenizer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [tokenizer(doc) for doc in cleaned_docs]\n",
    "dictionary = Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "\n",
    "# Extract topic words\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)]\n",
    "               for topic in range(len(set(topics)) - 1)]\n",
    "\n",
    "# Evaluate coherence\n",
    "coherence_model = CoherenceModel(topics=topic_words,\n",
    "                                 texts=tokens,\n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary,\n",
    "                                 coherence='c_v')\n",
    "coherence = coherence_model.get_coherence()\n",
    "\n",
    "print(f\"Coherence Score: {coherence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a format suitable for BERTopic\n",
    "# docs = []\n",
    "# for doc in doc_term:\n",
    "#     doc_str = ' '.join([dictionary[word_id] for word_id, _ in doc])\n",
    "#     docs.append(doc_str)\n",
    "\n",
    "# # Apply BERTopic\n",
    "# topic_model = bertopic.BERTopic(embedding_model='sentence-transformers/all-MiniLM-L6-v2',nr_topics=15,verbose=True)\n",
    "# topics, probabilities = topic_model.fit_transform(docs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info().to_csv(\"data/topic_info.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add topics to the original DataFrame\n",
    "data['Topic'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/clean_data/data_with_bertopic_column.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
