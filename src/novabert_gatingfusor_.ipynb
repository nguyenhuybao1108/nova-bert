{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import copy\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "embed_dim = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/baonguyen/IU/thesis/data/clean_data/data_with_bertopic_column.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else x), axis=0)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_date']=pd.to_datetime(df['review_date'])\n",
    "df_sorted = df.sort_values('review_date')\n",
    "df_sorted.rename(columns={'rented for':'rented_for','body type':'body_type','bust size':'bust_size'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_feature =  ['rented_for','bust_size','Topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item to index and vice versa\n",
    "unique_item_id = set(df_sorted['item_id'])\n",
    "item_to_index = {item:idx +1 for idx , item in enumerate(unique_item_id)}\n",
    "index_to_item = {idx+1:item for idx , item in enumerate(unique_item_id)}\n",
    "\n",
    "for i in side_feature:\n",
    "    exec(f'unique_{i} = set(df_sorted[i])')\n",
    "    exec(f'{i}_to_index = {{i:idx+1 for idx,i in enumerate(unique_{i})}}')\n",
    "    exec(f'index_to_{i} = {{idx+1:i for idx,i in enumerate(unique_{i})}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group and aggregate\n",
    "user_item_sequence = (\n",
    "    df_sorted.groupby('user_id')[['item_id']+side_feature]\n",
    "    .agg(list)\n",
    "    .to_dict(orient='index')\n",
    ")\n",
    "\n",
    "# Step 2: Remove users with fewer than 2 item_ids\n",
    "user_item_sequence = {\n",
    "    user: val\n",
    "    for user, val in user_item_sequence.items()\n",
    "    if len(val['item_id']) >= 2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_to_index_sequence = {}\n",
    "\n",
    "for user, value in user_item_sequence.items():\n",
    "    user_dict = {\n",
    "        'item_id': [item_to_index[item] for item in value['item_id']]\n",
    "    }\n",
    "    for i in side_feature:\n",
    "        # Dynamically get the correct mapping dict by name\n",
    "        mapping_dict = globals()[f\"{i}_to_index\"]\n",
    "        user_dict[i] = [mapping_dict[a] for a in value[i]]\n",
    "    user_item_to_index_sequence[user] = user_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def mask_sequence(sequence: dict, mask_ratio: float):\n",
    "    labels = {}\n",
    "    mask_seq = {}\n",
    "    for user, seq in sequence.items():\n",
    "        mask_seq[user] = copy.deepcopy(seq)  # Deep copy so original is untouched\n",
    "        labels[user] = [-100] * len(seq['item_id'])\n",
    "        for i in range(len(mask_seq[user]['item_id'])):\n",
    "            if random.random() < mask_ratio:\n",
    "                labels[user][i] = mask_seq[user]['item_id'][i]  # Save original item id\n",
    "                mask_seq[user]['item_id'][i] = 0       # Mask the item id\n",
    "                for feature in side_feature:\n",
    "                    mask_seq[user][feature][i] = 0\n",
    "                \n",
    "    return mask_seq, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_seq , labels = mask_sequence(user_item_to_index_sequence,mask_ratio=0.35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(mask_seq, labels, max_len=64, pad_item = 0, pad_label=-100):\n",
    "    \"\"\"\n",
    "    Pads all user sequences in mask_seq and labels to max_len.\n",
    "    \n",
    "    Args:\n",
    "        mask_seq: dict of user_id -> \n",
    "        labels: dict of user_id -> [...]\n",
    "        max_len: desired length after padding\n",
    "        pad_item: value for padding 'item_id'\n",
    "        pad_topic: value for padding 'Topic'\n",
    "        pad_label: value for padding labels\n",
    "\n",
    "    Returns:\n",
    "        padded_mask_seq, padded_labels (dicts)\n",
    "    \"\"\"\n",
    "    def pad(seq, max_len, pad_value):\n",
    "        if len(seq) < max_len:\n",
    "            return seq + [pad_value] * (max_len - len(seq))\n",
    "        else:\n",
    "            return seq[len(seq)-max_len:len(seq)]\n",
    "    \n",
    "    padded_mask_seq = {}\n",
    "    padded_labels = {}\n",
    "\n",
    "    for user in mask_seq:\n",
    "        padded_mask_seq[user] = {\n",
    "            **{'item_id': pad(mask_seq[user]['item_id'], max_len, pad_item)},\n",
    "            **{i: pad(mask_seq[user][i], max_len, pad_item) for i in side_feature}\n",
    "        }\n",
    "\n",
    "        padded_labels[user] = pad(labels[user], max_len, pad_label)\n",
    "    \n",
    "    return padded_mask_seq, padded_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_mask_seq,padded_labels = padding(mask_seq,labels,max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# class SinusoidalPositionalEncoding(nn.Module):\n",
    "#     def __init__(self, hidden_size, max_len=5000):\n",
    "#         super(SinusoidalPositionalEncoding, self).__init__()\n",
    "#         position = torch.arange(0, max_len).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, hidden_size, 2) * -(np.log(10000.0) / hidden_size))\n",
    "#         pe = torch.zeros(max_len, hidden_size)\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0)\n",
    "#         self.register_buffer('pe', pe)\n",
    "#     def forward(self, x):\n",
    "#         seq_len = x.size(1)\n",
    "#         return self.pe[:, :seq_len, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_item_ids = torch.stack([\n",
    "#     torch.tensor(user_seq['item_id']) for user_seq in padded_mask_seq.values()\n",
    "# ])\n",
    "# tensor_topic_ids = torch.stack([\n",
    "#     torch.tensor(user_seq['Topic']) for user_seq in padded_mask_seq.values()\n",
    "# ])\n",
    "# tensor_labels = torch.stack([\n",
    "#     torch.tensor(seq) for seq in padded_labels.values()\n",
    "#     ])\n",
    "# train_dataset = torch.utils.data.TensorDataset(\n",
    "#     tensor_item_ids,\n",
    "#     tensor_topic_ids,\n",
    "#     tensor_labels\n",
    "# )\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class differentiable_attn_mask(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(differentiable_attn_mask).__init__()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nova bert architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatingFusor(nn.Module):\n",
    "    def __init__(self, h):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(h, 1))\n",
    "\n",
    "    def forward(self, features):   \n",
    "        gates = torch.sigmoid(features @ self.weight)         \n",
    "        fused = torch.sum(gates * features, dim=2)            \n",
    "        return fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NovabertEmbedding(nn.Module):\n",
    "    def __init__(self,num_item,num_side_feature_ids:dict,embedding_dim,max_len=64):\n",
    "        super(NovabertEmbedding,self).__init__()\n",
    "        self.item_embedding = nn.Embedding(num_item+1,embedding_dim)\n",
    "        self.side_embedding_  = nn.ModuleDict()\n",
    "        for feat_name,num_feat in num_side_feature_ids.items():\n",
    "            self.side_embedding_[feat_name]=nn.Embedding(num_feat+1,embedding_dim)\n",
    "        self.position_encoding = nn.Embedding(max_len, embedding_dim)\n",
    "    def forward(self, item_ids, side_feature_ids: dict):\n",
    "        position_ids = torch.arange(item_ids.size(1), dtype=torch.long, device=item_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(item_ids)\n",
    "        item_embed = self.item_embedding(item_ids)\n",
    "        pos_embed = self.position_encoding(position_ids)\n",
    "        item_embed = item_embed + pos_embed\n",
    "        side_emb_list = []\n",
    "\n",
    "        for feat_name in self.side_embedding_:\n",
    "            \n",
    "            feat_ids = side_feature_ids[feat_name]   # <-- Access by key, get tensor\n",
    "            side_embed = self.side_embedding_[feat_name](feat_ids) + pos_embed\n",
    "            side_emb_list.append(side_embed)\n",
    "            \n",
    "        return item_embed, side_emb_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NovabertCrossAttention(nn.Module):\n",
    "    def __init__(self,embedding_dim,num_heads=8):\n",
    "        super(NovabertCrossAttention,self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim //num_heads\n",
    "        self.value_proj = nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.query_proj = nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.key_proj = nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.fusor = GatingFusor(h=embedding_dim)\n",
    "\n",
    "\n",
    "        self.output_proj = nn.Sequential(\n",
    "              \n",
    "        )\n",
    "    def forward(self,item_embed,side_feature_embed:list,attn_mask=None,key_padding_mask=None):\n",
    "        batch_size , sequence_len , embedding_dim = item_embed.size()\n",
    "        def reshape(x:torch.tensor):\n",
    "            return x.view(batch_size,sequence_len,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        # Batch,num_head,sequence_len,head_dim  (B,H,L,D)\n",
    "        # print(*side_feature_embed)\n",
    "        \n",
    "        \n",
    "        features = torch.stack([item_embed] + side_feature_embed, dim=2)\n",
    "\n",
    "        fused_features = self.fusor(features)\n",
    "        V = self.value_proj(item_embed)\n",
    "        Q = self.query_proj(fused_features)\n",
    "        K = self.key_proj(fused_features)\n",
    "        Q = reshape(Q)\n",
    "        K = reshape(K)\n",
    "        V = reshape(V)\n",
    "        scores = torch.matmul(Q,K.transpose(-2,-1)) / np.sqrt(self.head_dim) # B,H,L,L \n",
    "        \n",
    "        if attn_mask is not None:\n",
    "            scores += attn_mask.unsqueeze(0)  # Broadcast across batch ??? **********\n",
    "            pass\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2) # B,1,1,L\n",
    "            scores = scores.masked_fill(key_padding_mask,float('-inf'))\n",
    "            # print(scores)\n",
    "\n",
    "        attn_weights = torch.softmax(scores,dim=-1)\n",
    "        attn_weights = torch.nan_to_num(attn_weights, nan=0.0)\n",
    "        attn_output = torch.matmul(attn_weights,V) \n",
    "\n",
    "        # concat\n",
    "        attn_output = attn_output.transpose(1,2).contiguous().view(batch_size,sequence_len,embedding_dim)\n",
    "        return self.output_proj(attn_output)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NovabertLayer(nn.Module):\n",
    "    def __init__(self,embedding_dim,num_heads):\n",
    "        super(NovabertLayer,self).__init__()\n",
    "        self.cross_attn = NovabertCrossAttention(embedding_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embedding_dim * 4, embedding_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    def forward(self, id_embed,side_feature_embed:list , attention_mask=None,key_padding_mask = None):\n",
    "        guided = self.cross_attn(id_embed,side_feature_embed,attention_mask,key_padding_mask)\n",
    "        x = self.norm1(id_embed + self.dropout(guided))\n",
    "        x = self.norm2(x + self.dropout(self.ffn(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NovabertModel(nn.Module):\n",
    "    def __init__(self, num_items,num_side_feature_ids:dict, embedding_dim, max_len=64, num_layers=4, num_heads=8):\n",
    "        super(NovabertModel, self).__init__()\n",
    "        self.embedding = NovabertEmbedding(num_item=num_items,\n",
    "                                           num_side_feature_ids=num_side_feature_ids,\n",
    "                                           embedding_dim=embedding_dim,\n",
    "                                           max_len=max_len)\n",
    "        self.nova_layer = nn.ModuleList([\n",
    "            NovabertLayer(embedding_dim, num_heads) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(embedding_dim, num_items + 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, item_ids, side_feature_ids: dict, attention_mask=None, key_padding_mask=None):\n",
    "        x, y = self.embedding(item_ids, side_feature_ids)\n",
    "        for layer in self.nova_layer:\n",
    "            x = layer(x, y, attention_mask, key_padding_mask)\n",
    "        return self.output_layer(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_ratio(ground_truth:list,prediction:list,k:int):\n",
    "    hits = 0\n",
    "    total = len(ground_truth)\n",
    "    for gt_item, pred in zip(ground_truth, prediction):\n",
    "        \n",
    "        if gt_item in pred[:k]:\n",
    "            hits += 1\n",
    "    print(f'{hits=},{total=}')            \n",
    "    return hits / total\n",
    "\n",
    "# -------------------------\n",
    "def evaluate_model(model, val_item_sequences, k=10, max_len=64,  index_to_item=None, device='mps'):\n",
    "    \"\"\"\n",
    "    Evaluate model hit ratio@k on validation data.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model.\n",
    "        val_item_sequences: Dict of user_id -> {'item_id': [...], <feat1>: [...], <feat2>: [...], ...}\n",
    "        k: Top-k for hit ratio.\n",
    "        max_len: Sequence length for padding.\n",
    "        side_feature: List of feature names, e.g. ['Topic', 'category'].\n",
    "        index_to_item: Dict mapping item indices back to original item ids.\n",
    "        device: Device to run the model on.\n",
    "\n",
    "    Returns:\n",
    "        Hit ratio@k.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    ground_truths = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for user, seq in val_item_sequences.items():\n",
    "            item_seq = seq['item_id']\n",
    "            # Prepare input and target\n",
    "            input_items = item_seq[:-1]\n",
    "            target_item = item_seq[-1]\n",
    "            # Pad input sequence (handle all side features)\n",
    "            mask_seq = {user: {'item_id': input_items}}\n",
    "            for feat in (side_feature or []):\n",
    "                mask_seq[user][feat] = seq[feat][:-1]\n",
    "            padded_seq, _ = padding(\n",
    "                mask_seq=mask_seq,\n",
    "                labels={user: []},\n",
    "                max_len=max_len\n",
    "            )\n",
    "            padded_items = padded_seq[user]['item_id']\n",
    "            # Prepare side feature tensors as a dict\n",
    "            side_input_dict = {\n",
    "                feat: torch.tensor([padded_seq[user][feat]], dtype=torch.long).to(device)\n",
    "                for feat in (side_feature or [])\n",
    "            }\n",
    "            item_tensor = torch.tensor([padded_items], dtype=torch.long).to(device)\n",
    "            key_padding_mask = (item_tensor == 0)\n",
    "            # Model call\n",
    "            logits = model(item_tensor, side_input_dict, key_padding_mask=key_padding_mask)[:, min(len(item_seq)-1, max_len-1), :]\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            topk = torch.topk(probabilities, k=k).indices[0].tolist()\n",
    "            ground_truths.append(index_to_item[target_item])\n",
    "            predictions.append([index_to_item[i] for i in topk])\n",
    "    return hit_ratio(ground_truths, predictions, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def train_model(\n",
    "    train_users, val_users, fold_num,\n",
    "    user_item_to_index_sequence, side_feature, unique_item_id,\n",
    "    embedding_dim=256, max_len=max_len, num_layers=2, num_heads=4, mask_ratio=0.6\n",
    "):\n",
    "    train_user_item_to_index_sequence = {user: seq for user, seq in user_item_to_index_sequence.items() if user in train_users}\n",
    "    mask_seq, labels = mask_sequence(train_user_item_to_index_sequence, mask_ratio=mask_ratio)\n",
    "    padded_mask_seq, padded_labels = padding(mask_seq, labels, max_len=max_len)\n",
    "\n",
    "    tensor_item_ids = torch.stack([\n",
    "        torch.tensor(user_seq['item_id']) for user_seq in padded_mask_seq.values()\n",
    "    ])\n",
    "    tensor_side_feats = {\n",
    "        feat: torch.stack([\n",
    "            torch.tensor(user_seq[feat]) for user_seq in padded_mask_seq.values()\n",
    "        ])\n",
    "        for feat in side_feature\n",
    "    }\n",
    "\n",
    "    tensor_labels = torch.stack([\n",
    "        torch.tensor(seq) for seq in padded_labels.values()\n",
    "        ])\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        tensor_item_ids,\n",
    "        *(tensor_side_feats[feat] for feat in side_feature),\n",
    "        tensor_labels\n",
    "    )\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    num_side_feature_ids = {feat : len(globals()[f'unique_{feat}']) for feat in side_feature} \n",
    "    model = NovabertModel(len(unique_item_id),num_side_feature_ids=num_side_feature_ids,embedding_dim=embedding_dim,max_len=max_len,num_layers=num_layers,num_heads=num_heads).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    best_hr = 0\n",
    "\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Fold {fold_num} Epoch {epoch+1}\", unit=\"batch\"):\n",
    "            item_ids = batch[0]\n",
    "            side_ids = {feat: batch[i+1] for i, feat in enumerate(side_feature)}\n",
    "            labels = batch[-1]\n",
    "\n",
    "            # .to(device)\n",
    "            item_ids = item_ids.to(device)\n",
    "            side_ids = {feat: tensor.to(device) for feat, tensor in side_ids.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            key_padding_mask = (item_ids == 0)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(item_ids, side_ids, key_padding_mask=key_padding_mask)\n",
    "            loss = criterion(outputs.view(-1, len(unique_item_id)+1), labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss {epoch_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_item_sequences = {user: seq for user, seq in user_item_to_index_sequence.items() if user in val_users}\n",
    "        val_hr = evaluate_model(\n",
    "            model,\n",
    "            val_item_sequences,\n",
    "            k=10,\n",
    "            max_len=max_len,\n",
    "            index_to_item=index_to_item,\n",
    "            device=device\n",
    "        )\n",
    "        print(f\"Fold {fold_num} Epoch {epoch+1}, Validation HR@10: {val_hr:.4f}\")\n",
    "\n",
    "        if val_hr > best_hr:\n",
    "            best_hr = val_hr\n",
    "            save_path = f\"models_item_with_novabert_gatingfusor_/fold_{fold_num}\"\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"{save_path}/best_model.pth\")\n",
    "    return model, best_hr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import torch\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "user_list = list(user_item_sequence.keys())\n",
    "fold_results = {}\n",
    "\n",
    "for fold_num, (train_idx, val_idx) in enumerate(kf.split(user_list), 1):\n",
    "    print(f\"\\nStarting Fold {fold_num}...\")\n",
    "    train_users = [user_list[i] for i in train_idx]\n",
    "    val_users = [user_list[i] for i in val_idx]\n",
    "\n",
    "    # Train model on this fold\n",
    "    model, val_hr = train_model(\n",
    "        train_users=train_users,\n",
    "        val_users=val_users,\n",
    "        fold_num=fold_num,\n",
    "        user_item_to_index_sequence=user_item_to_index_sequence,\n",
    "        side_feature=side_feature,\n",
    "        unique_item_id=unique_item_id,\n",
    "    )\n",
    "    fold_results[fold_num] = val_hr\n",
    "\n",
    "    save_path = f\"results_item_with_novabert_gatingfusor_/fold_{fold_num}\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    with open(f\"{save_path}/results.txt\", \"w\") as f:\n",
    "        f.write(f\"Validation HR@10: {val_hr}\\n\")\n",
    "\n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    elif hasattr(torch, 'mps') and torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "with open(\"results_item_with_novabert_gatingfusor_/overall_results.txt\", \"w\") as f:\n",
    "    for fold, hr in fold_results.items():\n",
    "        f.write(f\"Fold {fold}: HR@10 = {hr}\\n\")\n",
    "    mean_hr = sum(fold_results.values()) / len(fold_results)\n",
    "    f.write(f\"\\nMean HR@10 across folds: {mean_hr}\\n\")\n",
    "\n",
    "print(f\"\\nMean HR@10 across folds: {mean_hr:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
