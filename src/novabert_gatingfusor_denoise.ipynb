{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import copy\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "embed_dim = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/baonguyen/IU/thesis/data/clean_data/data_with_bertopic_column.csv')\n",
    "# df['age'].fillna(df['age'].mode()[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_date']=pd.to_datetime(df['review_date'])\n",
    "df_sorted = df.sort_values('review_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item to index and vice versa\n",
    "unique_item_id = set(df_sorted['item_id'])\n",
    "item_to_index = {item:idx +1 for idx , item in enumerate(unique_item_id)}\n",
    "index_to_item = {idx+1:item for idx , item in enumerate(unique_item_id)}\n",
    "# topic to index and vice versa\n",
    "unique_Topic = set(df_sorted['Topic'])\n",
    "topic_to_index = {topic:idx+1 for idx , topic in enumerate(unique_Topic)}\n",
    "index_to_topic = {idx+1:topic for idx,topic in enumerate(unique_Topic)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group and aggregate\n",
    "user_item_sequence = (\n",
    "    df_sorted.groupby('user_id')[['item_id', 'Topic']]\n",
    "    .agg(list)\n",
    "    .to_dict(orient='index')\n",
    ")\n",
    "\n",
    "# Step 2: Remove users with fewer than 2 item_ids\n",
    "user_item_sequence = {\n",
    "    user: val\n",
    "    for user, val in user_item_sequence.items()\n",
    "    if len(val['item_id']) >= 2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_to_index_sequence = {}\n",
    "for user,value in user_item_sequence.items():\n",
    "    user_item_to_index_sequence[user] = {'item_id':[item_to_index[item] for item in value['item_id']],\n",
    "                                         'Topic':[topic_to_index[topic] for topic in value['Topic']]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def mask_sequence(sequence: dict, mask_ratio: float):\n",
    "    labels = {}\n",
    "    mask_seq = {}\n",
    "    for user, seq in sequence.items():\n",
    "        mask_seq[user] = copy.deepcopy(seq)  # Deep copy so original is untouched\n",
    "        labels[user] = [-100] * len(seq['item_id'])\n",
    "        for i in range(len(mask_seq[user]['item_id'])):\n",
    "            if random.random() < mask_ratio:\n",
    "                labels[user][i] = mask_seq[user]['item_id'][i]  # Save original item id\n",
    "                mask_seq[user]['item_id'][i] = 0       # Mask the item id\n",
    "                mask_seq[user]['Topic'][i] = 0\n",
    "    return mask_seq, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mask_seq , labels = mask_sequence(user_item_to_index_sequence,mask_ratio=0.35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(mask_seq, labels, max_len=64, pad_item=0, pad_topic=0, pad_label=-100):\n",
    "    \"\"\"\n",
    "    Pads all user sequences in mask_seq and labels to max_len.\n",
    "    \n",
    "    Args:\n",
    "        mask_seq: dict of user_id -> {'item_id': [...], 'Topic': [...]}\n",
    "        labels: dict of user_id -> [...]\n",
    "        max_len: desired length after padding\n",
    "        pad_item: value for padding 'item_id'\n",
    "        pad_topic: value for padding 'Topic'\n",
    "        pad_label: value for padding labels\n",
    "\n",
    "    Returns:\n",
    "        padded_mask_seq, padded_labels (dicts)\n",
    "    \"\"\"\n",
    "    def pad(seq, max_len, pad_value):\n",
    "        if len(seq) < max_len:\n",
    "            return seq + [pad_value] * (max_len - len(seq))\n",
    "        else:\n",
    "            return seq[:max_len]\n",
    "    \n",
    "    padded_mask_seq = {}\n",
    "    padded_labels = {}\n",
    "\n",
    "    for user in mask_seq:\n",
    "        padded_mask_seq[user] = {\n",
    "            'item_id': pad(mask_seq[user]['item_id'], max_len, pad_item),\n",
    "            'Topic': pad(mask_seq[user]['Topic'], max_len, pad_topic)\n",
    "        }\n",
    "        padded_labels[user] = pad(labels[user], max_len, pad_label)\n",
    "    \n",
    "    return padded_mask_seq, padded_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded_mask_seq,padded_labels = padding(mask_seq,labels,max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# class SinusoidalPositionalEncoding(nn.Module):\n",
    "#     def __init__(self, hidden_size, max_len=5000):\n",
    "#         super(SinusoidalPositionalEncoding, self).__init__()\n",
    "#         position = torch.arange(0, max_len).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, hidden_size, 2) * -(np.log(10000.0) / hidden_size))\n",
    "#         pe = torch.zeros(max_len, hidden_size)\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0)\n",
    "#         self.register_buffer('pe', pe)\n",
    "#     def forward(self, x):\n",
    "#         seq_len = x.size(1)\n",
    "#         return self.pe[:, :seq_len, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_item_ids = torch.stack([\n",
    "#     torch.tensor(user_seq['item_id']) for user_seq in padded_mask_seq.values()\n",
    "# ])\n",
    "# tensor_topic_ids = torch.stack([\n",
    "#     torch.tensor(user_seq['Topic']) for user_seq in padded_mask_seq.values()\n",
    "# ])\n",
    "# tensor_labels = torch.stack([\n",
    "#     torch.tensor(seq) for seq in padded_labels.values()\n",
    "#     ])\n",
    "# train_dataset = torch.utils.data.TensorDataset(\n",
    "#     tensor_item_ids,\n",
    "#     tensor_topic_ids,\n",
    "#     tensor_labels\n",
    "# )\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class differentiable_attn_mask(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(differentiable_attn_mask).__init__()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nova bert architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatingFusor(nn.Module):\n",
    "    def __init__(self, h):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(h, 1))\n",
    "\n",
    "    def forward(self, features):  # [B, T, 2, h]\n",
    "        gates = torch.sigmoid(features @ self.weight)         # [B, T, 2, 1]\n",
    "        fused = torch.sum(gates * features, dim=2)            # [B, T, h]\n",
    "        return fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NovabertCrossAttention(nn.Module):\n",
    "    def __init__(self,embedding_dim,num_heads=8):\n",
    "        super(NovabertCrossAttention,self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim //num_heads\n",
    "        self.value_proj = nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.query_proj = nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.key_proj = nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.fusor = GatingFusor(h=embedding_dim)\n",
    "\n",
    "\n",
    "        self.output_proj = nn.Sequential(\n",
    "            \n",
    "        )\n",
    "    def forward(self,item_embed,topic_embed,attn_mask=None,key_padding_mask=None):\n",
    "        batch_size , sequence_len , embedding_dim = item_embed.size()\n",
    "        def reshape(x:torch.tensor):\n",
    "            return x.view(batch_size,sequence_len,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        # Batch,num_head,sequence_len,head_dim  (B,H,L,D)\n",
    "        features = torch.stack([item_embed, topic_embed], dim=2)  # [B, T, 2, h]\n",
    "        fused_features = self.fusor(features)\n",
    "        V = self.value_proj(item_embed)\n",
    "        Q = self.query_proj(fused_features)\n",
    "        K = self.key_proj(fused_features)\n",
    "        Q = reshape(Q)\n",
    "        K = reshape(K)\n",
    "        V = reshape(V)\n",
    "        scores = torch.matmul(Q,K.transpose(-2,-1)) / np.sqrt(self.head_dim) # B,H,L,L \n",
    "        \n",
    "        if attn_mask is not None:\n",
    "            scores += attn_mask.unsqueeze(0)  # Broadcast across batch ??? **********\n",
    "            pass\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2) # B,1,1,L\n",
    "            scores = scores.masked_fill(key_padding_mask,float('-inf'))\n",
    "            # print(scores)\n",
    "\n",
    "        attn_weights = torch.softmax(scores,dim=-1)\n",
    "        attn_weights = torch.nan_to_num(attn_weights, nan=0.0)  # optional safety net\n",
    "        attn_output = torch.matmul(attn_weights,V) # B,H,L,D\n",
    "\n",
    "        # concat\n",
    "        attn_output = attn_output.transpose(1,2).contiguous().view(batch_size,sequence_len,embedding_dim)\n",
    "        return self.output_proj(attn_output)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NovabertLayer(nn.Module):\n",
    "    def __init__(self,embedding_dim,num_heads):\n",
    "        super(NovabertLayer,self).__init__()\n",
    "        self.cross_attn = NovabertCrossAttention(embedding_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embedding_dim * 4, embedding_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    def forward(self, id_embed, topic_embed, attention_mask=None,key_padding_mask = None):\n",
    "        guided = self.cross_attn(id_embed, topic_embed,attention_mask,key_padding_mask)\n",
    "        x = self.norm1(id_embed + self.dropout(guided))\n",
    "        x = self.norm2(x + self.dropout(self.ffn(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NovabertModel(nn.Module):\n",
    "    def __init__(self, num_items, num_topics, embedding_dim, max_len=64, num_layers=4, num_heads=8):\n",
    "        super(NovabertModel, self).__init__()\n",
    "        self.item_embedding = nn.Embedding(num_items + 1, embedding_dim)\n",
    "        self.topic_embedding = nn.Embedding(num_topics + 1, embedding_dim)\n",
    "        self.position_encoding = nn.Embedding(max_len, embedding_dim)\n",
    "\n",
    "        self.nova_layer = nn.ModuleList([\n",
    "            NovabertLayer(embedding_dim, num_heads) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(embedding_dim, num_items + 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, item_ids, topic_ids, attention_mask=None, key_padding_mask=None):\n",
    "        x = self.item_embedding(item_ids)  # [B, T, D]\n",
    "        y = self.topic_embedding(topic_ids)\n",
    "\n",
    "        # Create position indices\n",
    "        position_ids = torch.arange(item_ids.size(1), dtype=torch.long, device=item_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(item_ids)  # [B, T]\n",
    "\n",
    "        pos_emb = self.position_encoding(position_ids)\n",
    "\n",
    "        x = x + pos_emb\n",
    "        y = y + pos_emb\n",
    "        \n",
    "        for layer in self.nova_layer:\n",
    "            x = layer(x, y, attention_mask, key_padding_mask)\n",
    "        \n",
    "        return self.output_layer(x)\n",
    "    def get_hidden(self, item_ids, topic_ids, attention_mask=None, key_padding_mask=None):\n",
    "        x = self.item_embedding(item_ids)\n",
    "        y = self.topic_embedding(topic_ids)\n",
    "\n",
    "        position_ids = torch.arange(item_ids.size(1), dtype=torch.long, device=item_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(item_ids)\n",
    "        pos_emb = self.position_encoding(position_ids)\n",
    "\n",
    "        x = x + pos_emb\n",
    "        y = y + pos_emb\n",
    "        \n",
    "        for layer in self.nova_layer:\n",
    "            x = layer(x, y, attention_mask, key_padding_mask)\n",
    "        return x  # [B, T, D]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_ratio(ground_truth:list,prediction:list,k:int):\n",
    "    hits = 0\n",
    "    total = len(ground_truth)\n",
    "    for gt_item, pred in zip(ground_truth, prediction):\n",
    "        if gt_item in pred[:k]:\n",
    "            hits += 1\n",
    "            print(gt_item,pred)\n",
    "    return hits / total\n",
    "\n",
    "# -------------------------\n",
    "def evaluate_model(model, val_item_sequences, k=10):\n",
    "    model.eval()\n",
    "    device = 'mps'\n",
    "\n",
    "    ground_truths = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for user, seq in val_item_sequences.items():\n",
    "            item_seq = seq['item_id']\n",
    "            topic_seq = seq['Topic']\n",
    "\n",
    "\n",
    "            # Prepare input and target\n",
    "            input_items = item_seq[:-1]\n",
    "            input_topics = topic_seq[:-1]\n",
    "            target_item = item_seq[-1]\n",
    "\n",
    "            # Use your own padding utility to ensure correct length\n",
    "            padded_seq, _ = padding(\n",
    "                mask_seq={user: {'item_id': input_items, 'Topic': input_topics}},\n",
    "                labels={user: []},  # empty labels not needed here\n",
    "                max_len=max_len\n",
    "            )\n",
    "\n",
    "            padded_items = padded_seq[user]['item_id']\n",
    "            padded_topics = padded_seq[user]['Topic']\n",
    "\n",
    "            item_tensor = torch.tensor([padded_items], dtype=torch.long).to(device)\n",
    "            topic_tensor = torch.tensor([padded_topics], dtype=torch.long).to(device)\n",
    "            key_padding_mask = (item_tensor == 0)\n",
    "\n",
    "            logits = model(item_tensor, topic_tensor, key_padding_mask=key_padding_mask)[:,min(len(item_seq)-1,max_len-1),:]\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            # print(probabilities.size())\n",
    "\n",
    "\n",
    "\n",
    "            topk = torch.topk(probabilities, k=k).indices[0].tolist()\n",
    "            # print(topk)\n",
    "            ground_truths.append(index_to_item[target_item])\n",
    "            predictions.append([index_to_item[i] for i in topk])\n",
    "            # print(ground_truths)\n",
    "            # print(predictions)\n",
    "    return hit_ratio(ground_truths, predictions, k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian_frobenius_regularization(\n",
    "    model, item_ids, topic_ids, key_padding_mask, lambda_jacobian=0.1, num_samples=1\n",
    "):\n",
    "    x = model.get_hidden(item_ids, topic_ids, key_padding_mask=key_padding_mask)  # [B, T, D]\n",
    "    mask = ~key_padding_mask\n",
    "    x_masked = x[mask]  # [num_valid, D]\n",
    "    if x_masked.numel() == 0:\n",
    "        return torch.tensor(0.0, device=x.device, requires_grad=True)\n",
    "\n",
    "    # Get unique non-padding indices\n",
    "    used_item_indices = torch.unique(item_ids[mask])\n",
    "    used_item_indices = used_item_indices[used_item_indices != 0]  # Remove pad token\n",
    "\n",
    "    if used_item_indices.numel() == 0:\n",
    "        return torch.tensor(0.0, device=x.device, requires_grad=True)\n",
    "\n",
    "    # Get only the embeddings for items actually used\n",
    "    sub_embedding = model.item_embedding(used_item_indices)\n",
    "    sub_embedding.requires_grad_(True)\n",
    "\n",
    "    reg = 0.0\n",
    "    for _ in range(num_samples):\n",
    "        eta = torch.randn_like(x_masked)\n",
    "        J_eta = torch.autograd.grad(\n",
    "            outputs=x_masked,\n",
    "            inputs=sub_embedding,\n",
    "            grad_outputs=eta,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            allow_unused=True,\n",
    "        )[0]\n",
    "        if J_eta is not None:\n",
    "            reg += (J_eta ** 2).sum() / x_masked.size(0)\n",
    "    reg = reg / num_samples\n",
    "    return lambda_jacobian * reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "def train_model(train_users,val_users,fold_num):\n",
    "    train_user_item_to_index_sequence = {user: seq for user, seq in user_item_to_index_sequence.items() if user in train_users}\n",
    "    mask_seq , labels = mask_sequence(train_user_item_to_index_sequence,mask_ratio=0.6)\n",
    "    padded_mask_seq,padded_labels = padding(mask_seq,labels,max_len=max_len)\n",
    "\n",
    "    tensor_item_ids = torch.stack([\n",
    "        torch.tensor(user_seq['item_id']) for user_seq in padded_mask_seq.values()\n",
    "    ])\n",
    "\n",
    "    tensor_topic_ids = torch.stack([\n",
    "        torch.tensor(user_seq['Topic']) for user_seq in padded_mask_seq.values()\n",
    "    ])\n",
    "\n",
    "    tensor_labels = torch.stack([\n",
    "        torch.tensor(seq) for seq in padded_labels.values()\n",
    "        ])\n",
    "        \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        tensor_item_ids,\n",
    "        tensor_topic_ids,\n",
    "        tensor_labels\n",
    "    )\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
    "\n",
    "\n",
    "    # train model \n",
    "    device = 'mps'\n",
    "    model = NovabertModel(len(unique_item_id),len(unique_Topic),embedding_dim=256,max_len=max_len,num_layers=2,num_heads=4).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    best_hr = 0\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch in tqdm(train_dataloader,desc=f\"Fold {fold_num} Epoch {epoch+1}\", unit=\"batch\"):\n",
    "            item_ids , topic_ids , labels = batch\n",
    "            item_ids , topic_ids , labels = item_ids.to(device) , topic_ids.to(device),labels.to(device)\n",
    "            key_padding_mask = (item_ids == 0)\n",
    "            attn_mask = differentiable_attn_mask()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(item_ids,topic_ids,key_padding_mask=key_padding_mask)\n",
    "            # print(outputs.size())\n",
    "            \n",
    "            loss = criterion(outputs.view(-1, len(unique_item_id) + 1), labels.view(-1))\n",
    "            jacobian_loss = jacobian_frobenius_regularization(model, item_ids, topic_ids,key_padding_mask)\n",
    "            loss += jacobian_loss\n",
    "            loss.backward()\n",
    "            torch.mps.empty_cache()\n",
    "            optimizer.step()\n",
    "            # print(loss.item())\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss {epoch_loss}\")\n",
    "        # evaluate model\n",
    "        model.eval()\n",
    "        # val_user_item_sequences = {user: seq for user, seq in user_item_sequence.items() if user in val_users}\n",
    "        val_item_sequences = {user: seq for user, seq in user_item_to_index_sequence.items() if user in val_users}\n",
    "        val_hr = evaluate_model(model, val_item_sequences,k=10)\n",
    "        print(f\"Fold {fold_num} Epoch {epoch+1}, Validation HR@10: {val_hr}\")\n",
    "        if val_hr > best_hr:\n",
    "            best_hr = val_hr\n",
    "            save_path = f\"models_item_with_novabert_gatingfusor_denoise/fold_{fold_num}\"\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"{save_path}/best_model.pth\")\n",
    "    return model, best_hr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5,shuffle=True,random_state=42)\n",
    "user_list = list(user_item_sequence.keys())\n",
    "fold_results = {}\n",
    "for fold_num , (train_idx,val_idx) in enumerate(kf.split(user_list),1):\n",
    "    print(f\"\\nStarting Fold {fold_num}...\")\n",
    "    train_users = [user_list[i] for i in train_idx]\n",
    "    val_users = [user_list[i] for i in val_idx]\n",
    "    model,val_hr =  train_model(train_users, val_users, fold_num)\n",
    "    fold_results[fold_num] = val_hr\n",
    "    save_path = f\"results_item_with_novabert_gatingfusor_denoise/fold_{fold_num}\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    with open(f\"{save_path}/results.txt\", \"w\") as f:\n",
    "        f.write(f\"Validation HR@10: {val_hr}\\n\")\n",
    "    del model\n",
    "    torch.mps.empty_cache()\n",
    "with open(\"results_item_with_novabert_gatingfusor_denoise/overall_results.txt\", \"w\") as f:\n",
    "    for fold, hr in fold_results.items():\n",
    "        f.write(f\"Fold {fold}: HR@10 = {hr}\\n\")\n",
    "    mean_hr = sum(fold_results.values()) / len(fold_results)\n",
    "    f.write(f\"\\nMean HR@10 across folds: {mean_hr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "# model = NovabertModel(len(unique_item_id),len(unique_Topic),embedding_dim=256,max_len=max_len,num_layers=2,num_heads=4)\n",
    "# summary(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
